I"1<h1 id="on-the-convergence-of-sgd-with-adaptive-stepsizes">On the Convergence of SGD with Adaptive Stepsizes</h1>

<p>과목: Optimization
완료: No
자료: https://skkuurp2021spring.slack.com/files/U01P8C9DB4M/F03QEF3NSRZ/on_the_convergence_of_stochastic_gradient_descent.pdf
작성일시: 2022년 7월 26일 오후 9:28
핵심 내용: SGD</p>

<p><a href="https://skkuurp2021spring.slack.com/archives/C02UU122WRF/p1658400560416939">https://skkuurp2021spring.slack.com/archives/C02UU122WRF/p1658400560416939</a></p>

\[\|\nabla f(x) - \nabla g(x,\xi_{i})\|\]
:ET