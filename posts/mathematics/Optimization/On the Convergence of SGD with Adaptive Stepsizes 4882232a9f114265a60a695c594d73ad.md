---
# layout: default
title: Opt SGD Adative Convergence rate
nav_order: 1
description: "제발 그만해..."
grand_parent: Mathematics
parent: Optimization
tags: ["Mathematics", "Optimization"]
has_children: ture
permalink: /mathematics/opt/convergencerate
usemathjax: true
# has_toc: true
---
# On the Convergence of SGD with Adaptive Stepsizes

{: .no_toc }

<details open markdown="block">
  <summary>
    Table of contents
  </summary>
  {: .text-delta }
1. TOC
{:toc}
</details>


$$\| \nabla f(x) - \nabla g(x,\xi_{i}) \|$$
과목: Optimization
완료: No
자료: https://skkuurp2021spring.slack.com/files/U01P8C9DB4M/F03QEF3NSRZ/on_the_convergence_of_stochastic_gradient_descent.pdf
작성일시: 2022년 7월 26일 오후 9:28
핵심 내용: SGD

## 제목2
[https://skkuurp2021spring.slack.com/archives/C02UU122WRF/p1658400560416939](https://skkuurp2021spring.slack.com/archives/C02UU122WRF/p1658400560416939)

$$\| \nabla f(x) - \nabla g(x,\xi_{i}) \|$$


### 제목3

## 제목4

# 제목 5